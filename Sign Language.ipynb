{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sign Language Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General description:\n",
    "\n",
    "**Set Histogram:** The first step is to select an appropriate histogram that all the images are to be filtered with, this gurantees that the resulting images are appropriately visible and distinguishable. The user runs the code below and places their hand inside the rectangle, then they press \"C\" on their keyboard until they find a good histogram, finally the user must press \"S\" to save the histogram for later usage.\n",
    "\n",
    "**Create Gestures:** This portion essentially has two jobs: creating or reading from an SQL database file which contains all the registered gestures saved as tuples of (id, name/ text). The user is then asked to enter the id and the text of the new gesture, the camera is then started where an automatic gesture detection algorithm enables the program to take certain frames and save them as .jpg images in the user device storage for later training the model.\n",
    "\n",
    "**Dipslay Gestures:** Displays all the gestures created so far as a grid image.\n",
    "\n",
    "**Rotate Images:** Creates flipped instances of the saved gesture images, to augment the training set in the purpose of obtaining a larger training and validation sets for later.\n",
    "\n",
    "**Load Images:** Reads the folder containing all the gesture images \"/gestures\" and compresses them into a pickle file for easier access when training the model, the file structure is \"/gestures/-gesture_id-/-image_number_.jpg/\", this structure helps by allowing the program to automatically figure out the lable associated with each image. The labels are then saved into their own pickle file. Data is seperated into training and validation sets according to some seperation factor.\n",
    "\n",
    "**Recognition:** This portion is responsible for the actual testing of the model, it takes images from the camera, crops the, and then inputs them to the keras model, an evaluation is obtained for each frame, an evaluation is only considered viable if it has a probability higher than some threshold (70% in this code), once the same text has been predicted in a number of concurrent frames it is shown to the user in the input field for if they were to send it to someone else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def build_squares(img):\n",
    "    x, y, w, h = 420, 140, 10, 10\n",
    "    d = 10\n",
    "    imgCrop = None\n",
    "    crop = None\n",
    "    for i in range(10):\n",
    "        for j in range(5):\n",
    "            if np.any(imgCrop == None):\n",
    "                imgCrop = img[y:y+h, x:x+w]\n",
    "            else:\n",
    "                imgCrop = np.hstack((imgCrop, img[y:y+h, x:x+w]))\n",
    "            #print(imgCrop.shape)\n",
    "            cv2.rectangle(img, (x,y), (x+w, y+h), (0,255,0), 1)\n",
    "            x+=w+d\n",
    "        if np.any(crop == None):\n",
    "            crop = imgCrop\n",
    "        else:\n",
    "            crop = np.vstack((crop, imgCrop)) \n",
    "        imgCrop = None\n",
    "        x = 420\n",
    "        y+=h+d\n",
    "    return crop\n",
    "\n",
    "def get_hand_hist():\n",
    "    cam = cv2.VideoCapture(1)\n",
    "    if cam.read()[0]==False:\n",
    "        cam = cv2.VideoCapture(0)\n",
    "    x, y, w, h = 300, 100, 300, 300\n",
    "    flagPressedC, flagPressedS = False, False\n",
    "    imgCrop = None\n",
    "    while True:\n",
    "        img = cam.read()[1]\n",
    "        img = cv2.flip(img, 1)\n",
    "        img = cv2.resize(img, (640, 480))\n",
    "        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "        \n",
    "        keypress = cv2.waitKey(1)\n",
    "        if keypress == ord('c'):\n",
    "            hsvCrop = cv2.cvtColor(imgCrop, cv2.COLOR_BGR2HSV)\n",
    "            flagPressedC = True\n",
    "            hist = cv2.calcHist([hsvCrop], [0, 1], None, [180, 256], [0, 180, 0, 256])\n",
    "            cv2.normalize(hist, hist, 0, 255, cv2.NORM_MINMAX)\n",
    "        elif keypress == ord('s'):\n",
    "            flagPressedS = True\t\n",
    "            break\n",
    "        if flagPressedC:\n",
    "            dst = cv2.calcBackProject([hsv], [0, 1], hist, [0, 180, 0, 256], 1)\n",
    "            dst1 = dst.copy()\n",
    "            disc = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(10,10))\n",
    "            cv2.filter2D(dst,-1,disc,dst)\n",
    "            blur = cv2.GaussianBlur(dst, (11,11), 0)\n",
    "            blur = cv2.medianBlur(blur, 15)\n",
    "            ret,thresh = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "            thresh = cv2.merge((thresh,thresh,thresh))\n",
    "            #cv2.imshow(\"res\", res)\n",
    "            cv2.imshow(\"Thresh\", thresh)\n",
    "        if not flagPressedS:\n",
    "            imgCrop = build_squares(img)\n",
    "        #cv2.rectangle(img, (x,y), (x+w, y+h), (0,255,0), 2)\n",
    "        cv2.imshow(\"Set hand histogram\", img)\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    with open(\"hist\", \"wb\") as f:\n",
    "        pickle.dump(hist, f)\n",
    "    print(\"Histogram Saved.\")\n",
    "\n",
    "\n",
    "get_hand_hist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Gestures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pickle, os, sqlite3, random\n",
    "\n",
    "image_x, image_y = 50, 50\n",
    "\n",
    "def get_hand_hist():\n",
    "    with open(\"hist\", \"rb\") as f:\n",
    "        hist = pickle.load(f)\n",
    "    return hist\n",
    "\n",
    "def init_create_folder_database():\n",
    "    # create the folder and database if not existent\n",
    "    if not os.path.exists(\"gestures\"):\n",
    "        os.mkdir(\"gestures\")\n",
    "    if not os.path.exists(\"gesture_db.db\"):\n",
    "        conn = sqlite3.connect(\"gesture_db.db\")\n",
    "        create_table_cmd = \"CREATE TABLE gesture ( g_id INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE, g_name TEXT NOT NULL )\"\n",
    "        conn.execute(create_table_cmd)\n",
    "        conn.commit()\n",
    "\n",
    "def create_folder(folder_name):\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.mkdir(folder_name)\n",
    "\n",
    "def store_in_db(g_id, g_name):\n",
    "    conn = sqlite3.connect(\"gesture_db.db\")\n",
    "    cmd = \"INSERT INTO gesture (g_id, g_name) VALUES (%s, \\'%s\\')\" % (g_id, g_name)\n",
    "    try:\n",
    "        conn.execute(cmd)\n",
    "    except sqlite3.IntegrityError:\n",
    "        choice = input(\"g_id already exists. Want to change the record? (y/n): \")\n",
    "        if choice.lower() == 'y':\n",
    "            cmd = \"UPDATE gesture SET g_name = \\'%s\\' WHERE g_id = %s\" % (g_name, g_id)\n",
    "            conn.execute(cmd)\n",
    "        else:\n",
    "            print(\"Doing nothing...\")\n",
    "            return\n",
    "    conn.commit()\n",
    "    \n",
    "def store_images(g_id):\n",
    "    total_pics = 1200\n",
    "    hist = get_hand_hist()\n",
    "    cam = cv2.VideoCapture(1)\n",
    "    if cam.read()[0]==False:\n",
    "        cam = cv2.VideoCapture(0)\n",
    "    x, y, w, h = 300, 100, 300, 300\n",
    "\n",
    "    create_folder(\"gestures/\"+str(g_id))\n",
    "    pic_no = 0\n",
    "    flag_start_capturing = False\n",
    "    frames = 0\n",
    "    \n",
    "    while True:\n",
    "        img = cam.read()[1]\n",
    "        img = cv2.flip(img, 1)\n",
    "        imgHSV = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "        dst = cv2.calcBackProject([imgHSV], [0, 1], hist, [0, 180, 0, 256], 1)\n",
    "        disc = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(10,10))\n",
    "        cv2.filter2D(dst,-1,disc,dst)\n",
    "        blur = cv2.GaussianBlur(dst, (11,11), 0)\n",
    "        blur = cv2.medianBlur(blur, 15)\n",
    "        thresh = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)[1]\n",
    "        thresh = cv2.merge((thresh,thresh,thresh))\n",
    "        thresh = cv2.cvtColor(thresh, cv2.COLOR_BGR2GRAY)\n",
    "        thresh = thresh[y:y+h, x:x+w]\n",
    "        contours = cv2.findContours(thresh.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)[0]\n",
    "        \"\"\"print(contours[1])\n",
    "        #print(contours[0])\n",
    "        for i in range(len(contours[0])):\n",
    "            print(cv2.contourArea(contours[0][0]))\"\"\"\n",
    "        #x = xd\n",
    "        if len(contours) > 0:\n",
    "            contour = max(contours, key = cv2.contourArea)\n",
    "            if cv2.contourArea(contour) > 10000 and frames > 50:\n",
    "                x1, y1, w1, h1 = cv2.boundingRect(contour)\n",
    "                pic_no += 1\n",
    "                save_img = thresh[y1:y1+h1, x1:x1+w1]\n",
    "                if w1 > h1:\n",
    "                    save_img = cv2.copyMakeBorder(save_img, int((w1-h1)/2) , int((w1-h1)/2) , 0, 0, cv2.BORDER_CONSTANT, (0, 0, 0))\n",
    "                elif h1 > w1:\n",
    "                    save_img = cv2.copyMakeBorder(save_img, 0, 0, int((h1-w1)/2) , int((h1-w1)/2) , cv2.BORDER_CONSTANT, (0, 0, 0))\n",
    "                save_img = cv2.resize(save_img, (image_x, image_y))\n",
    "                rand = random.randint(0, 10)\n",
    "                if rand % 2 == 0:\n",
    "                    save_img = cv2.flip(save_img, 1)\n",
    "                cv2.putText(img, \"Capturing...\", (30, 60), cv2.FONT_HERSHEY_TRIPLEX, 2, (127, 255, 255))\n",
    "                cv2.imwrite(\"gestures/\"+str(g_id)+\"/\"+str(pic_no)+\".jpg\", save_img)\n",
    "\n",
    "        cv2.rectangle(img, (x,y), (x+w, y+h), (0,255,0), 2)\n",
    "        cv2.putText(img, str(pic_no), (30, 400), cv2.FONT_HERSHEY_TRIPLEX, 1.5, (127, 127, 255))\n",
    "        cv2.imshow(\"Capturing gesture\", img)\n",
    "        cv2.imshow(\"thresh\", thresh)\n",
    "        keypress = cv2.waitKey(1)\n",
    "        if keypress == ord('c'):\n",
    "            if flag_start_capturing == False:\n",
    "                flag_start_capturing = True\n",
    "            else:\n",
    "                flag_start_capturing = False\n",
    "                frames = 0\n",
    "        if flag_start_capturing == True:\n",
    "            frames += 1\n",
    "        if pic_no == total_pics:\n",
    "            break\n",
    "\n",
    "init_create_folder_database()\n",
    "\n",
    "g_id = input(\"Enter gesture no.: \")\n",
    "g_name = input(\"Enter gesture name/text: \")\n",
    "store_in_db(g_id, g_name)\n",
    "store_images(g_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Gestures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, os, random\n",
    "import numpy as np\n",
    "\n",
    "def get_image_size():\n",
    "    img = cv2.imread('gestures/10/100.jpg', 0)\n",
    "    return img.shape\n",
    "\n",
    "gestures = os.listdir('gestures/')\n",
    "gestures.sort(key = int)\n",
    "begin_index = 0\n",
    "end_index = 5\n",
    "image_x, image_y = get_image_size()\n",
    "\n",
    "if len(gestures)%5 != 0:\n",
    "    rows = int(len(gestures)/5)+1\n",
    "else:\n",
    "    rows = int(len(gestures)/5)\n",
    "\n",
    "full_img = None\n",
    "for i in range(rows):\n",
    "    col_img = None\n",
    "    for j in range(begin_index, end_index):\n",
    "        img_path = \"gestures/%s/%d.jpg\" % (j, random.randint(1, 1200))\n",
    "        img = cv2.imread(img_path, 0)\n",
    "        if np.any(img == None):\n",
    "            img = np.zeros((image_y, image_x), dtype = np.uint8)\n",
    "        if np.any(col_img == None):\n",
    "            col_img = img\n",
    "        else:\n",
    "            col_img = np.hstack((col_img, img))\n",
    "\n",
    "    begin_index += 5\n",
    "    end_index += 5\n",
    "    if np.any(full_img == None):\n",
    "        full_img = col_img\n",
    "    else:\n",
    "        full_img = np.vstack((full_img, col_img))\n",
    "\n",
    "\n",
    "cv2.imshow(\"gestures\", full_img)\n",
    "cv2.imwrite('full_img.jpg', full_img)\n",
    "cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotate Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, os\n",
    "\n",
    "def flip_images():\n",
    "    gest_folder = \"gestures\"\n",
    "    images_labels = []\n",
    "    images = []\n",
    "    labels = []\n",
    "    for g_id in os.listdir(gest_folder):\n",
    "        for i in range(1200):\n",
    "            path = gest_folder+\"/\"+g_id+\"/\"+str(i+1)+\".jpg\"\n",
    "            new_path = gest_folder+\"/\"+g_id+\"/\"+str(i+1+1200)+\".jpg\"\n",
    "            print(path)\n",
    "            img = cv2.imread(path, 0)\n",
    "            img = cv2.flip(img, 1)\n",
    "            cv2.imwrite(new_path, img)\n",
    "\n",
    "flip_images()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def pickle_images_labels():\n",
    "    images_labels = []\n",
    "    images = glob(\"gestures/*/*.jpg\")\n",
    "    images.sort()\n",
    "    for image in images:\n",
    "        print(image)\n",
    "        label = image[image.find(os.sep)+1: image.rfind(os.sep)]\n",
    "        img = cv2.imread(image, 0)\n",
    "        images_labels.append((np.array(img, dtype=np.uint8), int(label)))\n",
    "    return images_labels\n",
    "\n",
    "images_labels = pickle_images_labels()\n",
    "images_labels = shuffle(shuffle(shuffle(shuffle(images_labels))))\n",
    "images, labels = zip(*images_labels)\n",
    "print(\"Length of images_labels\", len(images_labels))\n",
    "\n",
    "train_images = images[:int(5/6*len(images))]\n",
    "print(\"Length of train_images\", len(train_images))\n",
    "with open(\"train_images\", \"wb\") as f:\n",
    "    pickle.dump(train_images, f)\n",
    "del train_images\n",
    "\n",
    "train_labels = labels[:int(5/6*len(labels))]\n",
    "print(\"Length of train_labels\", len(train_labels))\n",
    "with open(\"train_labels\", \"wb\") as f:\n",
    "    pickle.dump(train_labels, f)\n",
    "del train_labels\n",
    "\n",
    "test_images = images[int(5/6*len(images)):int(11/12*len(images))]\n",
    "print(\"Length of test_images\", len(test_images))\n",
    "with open(\"test_images\", \"wb\") as f:\n",
    "    pickle.dump(test_images, f)\n",
    "del test_images\n",
    "\n",
    "test_labels = labels[int(5/6*len(labels)):int(11/12*len(images))]\n",
    "print(\"Length of test_labels\", len(test_labels))\n",
    "with open(\"test_labels\", \"wb\") as f:\n",
    "    pickle.dump(test_labels, f)\n",
    "del test_labels\n",
    "\n",
    "val_images = images[int(11/12*len(images)):]\n",
    "print(\"Length of test_images\", len(val_images))\n",
    "with open(\"val_images\", \"wb\") as f:\n",
    "    pickle.dump(val_images, f)\n",
    "del val_images\n",
    "\n",
    "val_labels = labels[int(11/12*len(labels)):]\n",
    "print(\"Length of val_labels\", len(val_labels))\n",
    "with open(\"val_labels\", \"wb\") as f:\n",
    "    pickle.dump(val_labels, f)\n",
    "del val_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import cv2, os\n",
    "from glob import glob\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import backend as K\n",
    "#K.set_image_dim_ordering('tf')\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "def get_image_size():\n",
    "    img = cv2.imread('gestures/10/100.jpg', 0)\n",
    "    return img.shape\n",
    "\n",
    "def get_num_of_classes():\n",
    "    return len(glob('gestures/*'))\n",
    "\n",
    "image_x, image_y = get_image_size()\n",
    "filepath=\"./cnn_model_keras9.h5\"\n",
    "def cnn_model():\n",
    "    num_of_classes = get_num_of_classes()\n",
    "    print(\"Classes: \",num_of_classes)\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, (2,2), input_shape=(image_x, image_y, 1), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
    "    model.add(Conv2D(32, (3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=(3, 3), padding='same'))\n",
    "    model.add(Conv2D(64, (5,5), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(5, 5), strides=(5, 5), padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_of_classes, activation='softmax'))\n",
    "    sgd = optimizers.SGD(lr=1e-2)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    \n",
    "    checkpoint1 = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint1]\n",
    "    #from keras.utils import plot_model\n",
    "    #plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model, callbacks_list\n",
    "\n",
    "def train():\n",
    "    with open(\"train_images\", \"rb\") as f:\n",
    "        train_images = np.array(pickle.load(f))\n",
    "    with open(\"train_labels\", \"rb\") as f:\n",
    "        train_labels = np.array(pickle.load(f), dtype=np.int32)\n",
    "\n",
    "    with open(\"val_images\", \"rb\") as f:\n",
    "        val_images = np.array(pickle.load(f))\n",
    "    with open(\"val_labels\", \"rb\") as f:\n",
    "        val_labels = np.array(pickle.load(f), dtype=np.int32)\n",
    "\n",
    "    #print(\"Labels: \", val_labels)\n",
    "    train_images = np.reshape(train_images, (train_images.shape[0], image_x, image_y, 1))\n",
    "    val_images = np.reshape(val_images, (val_images.shape[0], image_x, image_y, 1))\n",
    "    train_labels[:] = train_labels[:]%10\n",
    "    val_labels[:] = val_labels[:]%10\n",
    "    train_labels = np_utils.to_categorical(train_labels)\n",
    "    val_labels = np_utils.to_categorical(val_labels)\n",
    "\n",
    "    #print(val_labels.shape)\n",
    "\n",
    "    model, callbacks_list = cnn_model()\n",
    "    model.summary()\n",
    "    #print(train_images.shape)\n",
    "    model.fit(train_images, train_labels, validation_data=(val_images, val_labels), epochs=5, batch_size=500, callbacks=callbacks_list)\n",
    "    scores = model.evaluate(val_images, val_labels, verbose=0)\n",
    "    model.save(filepath)\n",
    "    print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))\n",
    "\n",
    "train()\n",
    "K.clear_session();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#from cnn_tf import cnn_model_fn\n",
    "import os\n",
    "import sqlite3, pyttsx3\n",
    "from tensorflow.keras.models import load_model\n",
    "from threading import Thread\n",
    "\n",
    "engine = pyttsx3.init()\n",
    "engine.setProperty('rate', 150)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "model = load_model('cnn_model_keras9.h5')\n",
    "\n",
    "def get_hand_hist():\n",
    "    with open(\"hist\", \"rb\") as f:\n",
    "        hist = pickle.load(f)\n",
    "    return hist\n",
    "\n",
    "def get_image_size():\n",
    "    img = cv2.imread('gestures/10/100.jpg', 0)\n",
    "    return img.shape\n",
    "\n",
    "image_x, image_y = get_image_size()\n",
    "\n",
    "def keras_process_image(img):\n",
    "    img = cv2.resize(img, (image_x, image_y))\n",
    "    img = np.array(img, dtype=np.float32)\n",
    "    img = np.reshape(img, (1, image_x, image_y, 1))\n",
    "    return img\n",
    "\n",
    "def keras_predict(model, image):\n",
    "    processed = keras_process_image(image)\n",
    "    pred_probab = model.predict(processed)[0]\n",
    "    pred_class = list(pred_probab).index(max(pred_probab))\n",
    "    return max(pred_probab), pred_class\n",
    "\n",
    "def get_pred_text_from_db(pred_class):\n",
    "    conn = sqlite3.connect(\"gesture_db.db\")\n",
    "    cmd = \"SELECT g_name FROM gesture WHERE g_id=\"+str(pred_class)\n",
    "    cursor = conn.execute(cmd)\n",
    "    for row in cursor:\n",
    "        return row[0]\n",
    "\n",
    "def get_pred_from_contour(contour, thresh):\n",
    "    x1, y1, w1, h1 = cv2.boundingRect(contour)\n",
    "    save_img = thresh[y1:y1+h1, x1:x1+w1]\n",
    "    text = \"\"\n",
    "    if w1 > h1:\n",
    "        save_img = cv2.copyMakeBorder(save_img, int((w1-h1)/2) , int((w1-h1)/2) , 0, 0, cv2.BORDER_CONSTANT, (0, 0, 0))\n",
    "    elif h1 > w1:\n",
    "        save_img = cv2.copyMakeBorder(save_img, 0, 0, int((h1-w1)/2) , int((h1-w1)/2) , cv2.BORDER_CONSTANT, (0, 0, 0))\n",
    "    pred_probab, pred_class = keras_predict(model, save_img)\n",
    "    if pred_probab*100 > 70:\n",
    "        text = get_pred_text_from_db(pred_class)\n",
    "    return text\n",
    "\n",
    "def get_operator(pred_text):\n",
    "    try:\n",
    "        pred_text = int(pred_text)\n",
    "    except:\n",
    "        return \"\"\n",
    "    operator = \"\"\n",
    "    if pred_text == 1:\n",
    "        operator = \"+\"\n",
    "    elif pred_text == 2:\n",
    "        operator = \"-\"\n",
    "    elif pred_text == 3:\n",
    "        operator = \"*\"\n",
    "    elif pred_text == 4:\n",
    "        operator = \"/\"\n",
    "    elif pred_text == 5:\n",
    "        operator = \"%\"\n",
    "    elif pred_text == 6:\n",
    "        operator = \"**\"\n",
    "    elif pred_text == 7:\n",
    "        operator = \">>\"\n",
    "    elif pred_text == 8:\n",
    "        operator = \"<<\"\n",
    "    elif pred_text == 9:\n",
    "        operator = \"&\"\n",
    "    elif pred_text == 0:\n",
    "        operator = \"|\"\n",
    "    return operator\n",
    "\n",
    "hist = get_hand_hist()\n",
    "x, y, w, h = 300, 100, 300, 300\n",
    "is_voice_on = True\n",
    "\n",
    "def get_img_contour_thresh(img):\n",
    "    img = cv2.flip(img, 1)\n",
    "    imgHSV = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    dst = cv2.calcBackProject([imgHSV], [0, 1], hist, [0, 180, 0, 256], 1)\n",
    "    disc = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(10,10))\n",
    "    cv2.filter2D(dst,-1,disc,dst)\n",
    "    blur = cv2.GaussianBlur(dst, (11,11), 0)\n",
    "    blur = cv2.medianBlur(blur, 15)\n",
    "    thresh = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)[1]\n",
    "    thresh = cv2.merge((thresh,thresh,thresh))\n",
    "    thresh = cv2.cvtColor(thresh, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = thresh[y:y+h, x:x+w]\n",
    "    contours = cv2.findContours(thresh.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)[0]\n",
    "    return img, contours, thresh\n",
    "\n",
    "def say_text(text):\n",
    "    if not is_voice_on:\n",
    "        return\n",
    "    while engine._inLoop:\n",
    "        pass\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "def calculator_mode(cam):\n",
    "    global is_voice_on\n",
    "    flag = {\"first\": False, \"operator\": False, \"second\": False, \"clear\": False}\n",
    "    count_same_frames = 0\n",
    "    first, operator, second = \"\", \"\", \"\"\n",
    "    pred_text = \"\"\n",
    "    calc_text = \"\"\n",
    "    info = \"Enter first number\"\n",
    "    Thread(target=say_text, args=(info,)).start()\n",
    "    count_clear_frames = 0\n",
    "    while True:\n",
    "        img = cam.read()[1]\n",
    "        img = cv2.resize(img, (640, 480))\n",
    "        img, contours, thresh = get_img_contour_thresh(img)\n",
    "        old_pred_text = pred_text\n",
    "        if len(contours) > 0:\n",
    "            contour = max(contours, key = cv2.contourArea)\n",
    "            if cv2.contourArea(contour) > 10000:\n",
    "                pred_text = get_pred_from_contour(contour, thresh)\n",
    "                if old_pred_text == pred_text:\n",
    "                    count_same_frames += 1\n",
    "                else:\n",
    "                    count_same_frames = 0\n",
    "\n",
    "                if pred_text == \"C\":\n",
    "                    if count_same_frames > 5:\n",
    "                        count_same_frames = 0\n",
    "                        first, second, operator, pred_text, calc_text = '', '', '', '', ''\n",
    "                        flag['first'], flag['operator'], flag['second'], flag['clear'] = False, False, False, False\n",
    "                        info = \"Enter first number\"\n",
    "                        Thread(target=say_text, args=(info,)).start()\n",
    "\n",
    "                elif pred_text == \"Best of Luck \" and count_same_frames > 15:\n",
    "                    count_same_frames = 0\n",
    "                    if flag['clear']:\n",
    "                        first, second, operator, pred_text, calc_text = '', '', '', '', ''\n",
    "                        flag['first'], flag['operator'], flag['second'], flag['clear'] = False, False, False, False\n",
    "                        info = \"Enter first number\"\n",
    "                        Thread(target=say_text, args=(info,)).start()\n",
    "                    elif second != '':\n",
    "                        flag['second'] = True\n",
    "                        info = \"Clear screen\"\n",
    "                        #Thread(target=say_text, args=(info,)).start()\n",
    "                        second = ''\n",
    "                        flag['clear'] = True\n",
    "                        try:\n",
    "                            calc_text += \"= \"+str(eval(calc_text))\n",
    "                        except:\n",
    "                            calc_text = \"Invalid operation\"\n",
    "                        if is_voice_on:\n",
    "                            speech = calc_text\n",
    "                            speech = speech.replace('-', ' minus ')\n",
    "                            speech = speech.replace('/', ' divided by ')\n",
    "                            speech = speech.replace('**', ' raised to the power ')\n",
    "                            speech = speech.replace('*', ' multiplied by ')\n",
    "                            speech = speech.replace('%', ' mod ')\n",
    "                            speech = speech.replace('>>', ' bitwise right shift ')\n",
    "                            speech = speech.replace('<<', ' bitwise leftt shift ')\n",
    "                            speech = speech.replace('&', ' bitwise and ')\n",
    "                            speech = speech.replace('|', ' bitwise or ')\n",
    "                            Thread(target=say_text, args=(speech,)).start()\n",
    "                    elif first != '':\n",
    "                        flag['first'] = True\n",
    "                        info = \"Enter operator\"\n",
    "                        Thread(target=say_text, args=(info,)).start()\n",
    "                        first = ''\n",
    "\n",
    "                elif pred_text != \"Best of Luck \" and pred_text.isnumeric():\n",
    "                    if flag['first'] == False:\n",
    "                        if count_same_frames > 15:\n",
    "                            count_same_frames = 0\n",
    "                            Thread(target=say_text, args=(pred_text,)).start()\n",
    "                            first += pred_text\n",
    "                            calc_text += pred_text\n",
    "                    elif flag['operator'] == False:\n",
    "                        operator = get_operator(pred_text)\n",
    "                        if count_same_frames > 15:\n",
    "                            count_same_frames = 0\n",
    "                            flag['operator'] = True\n",
    "                            calc_text += operator\n",
    "                            info = \"Enter second number\"\n",
    "                            Thread(target=say_text, args=(info,)).start()\n",
    "                            operator = ''\n",
    "                    elif flag['second'] == False:\n",
    "                        if count_same_frames > 15:\n",
    "                            Thread(target=say_text, args=(pred_text,)).start()\n",
    "                            second += pred_text\n",
    "                            calc_text += pred_text\n",
    "                            count_same_frames = 0\t\n",
    "\n",
    "        if count_clear_frames == 30:\n",
    "            first, second, operator, pred_text, calc_text = '', '', '', '', ''\n",
    "            flag['first'], flag['operator'], flag['second'], flag['clear'] = False, False, False, False\n",
    "            info = \"Enter first number\"\n",
    "            Thread(target=say_text, args=(info,)).start()\n",
    "            count_clear_frames = 0\n",
    "\n",
    "        blackboard = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "        cv2.putText(blackboard, \"Calculator Mode\", (100, 50), cv2.FONT_HERSHEY_TRIPLEX, 1.5, (255, 0,0))\n",
    "        cv2.putText(blackboard, \"Predicted text- \" + pred_text, (30, 100), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 255, 0))\n",
    "        cv2.putText(blackboard, \"Operator \" + operator, (30, 140), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 255, 127))\n",
    "        cv2.putText(blackboard, calc_text, (30, 240), cv2.FONT_HERSHEY_TRIPLEX, 2, (255, 255, 255))\n",
    "        cv2.putText(blackboard, info, (30, 440), cv2.FONT_HERSHEY_TRIPLEX, 1, (0, 255, 255) )\n",
    "        if is_voice_on:\n",
    "            cv2.putText(blackboard, \"Voice ON\", (450, 440), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 127, 0))\n",
    "        else:\n",
    "            cv2.putText(blackboard, \"Voice OFF\", (450, 440), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 127, 0))\n",
    "        cv2.rectangle(img, (x,y), (x+w, y+h), (0,255,0), 2)\n",
    "        res = np.hstack((img, blackboard))\n",
    "        cv2.imshow(\"Recognizing gesture\", res)\n",
    "        cv2.imshow(\"thresh\", thresh)\n",
    "        keypress = cv2.waitKey(1)\n",
    "        if keypress == ord('q') or keypress == ord('t'):\n",
    "            break\n",
    "        if keypress == ord('v') and is_voice_on:\n",
    "            is_voice_on = False\n",
    "        elif keypress == ord('v') and not is_voice_on:\n",
    "            is_voice_on = True\n",
    "\n",
    "    if keypress == ord('t'):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def text_mode(cam):\n",
    "    global is_voice_on\n",
    "    text = \"\"\n",
    "    word = \"\"\n",
    "    count_same_frame = 0\n",
    "    while True:\n",
    "        img = cam.read()[1]\n",
    "        img = cv2.resize(img, (640, 480))\n",
    "        img, contours, thresh = get_img_contour_thresh(img)\n",
    "        old_text = text\n",
    "        if len(contours) > 0:\n",
    "            contour = max(contours, key = cv2.contourArea)\n",
    "            if cv2.contourArea(contour) > 10000:\n",
    "                text = get_pred_from_contour(contour, thresh)\n",
    "                if old_text == text:\n",
    "                    count_same_frame += 1\n",
    "                else:\n",
    "                    count_same_frame = 0\n",
    "\n",
    "                if count_same_frame > 20:\n",
    "                    if len(text) == 1:\n",
    "                        Thread(target=say_text, args=(text, )).start()\n",
    "                    word = word + text\n",
    "                    if word.startswith('I/Me '):\n",
    "                        word = word.replace('I/Me ', 'I ')\n",
    "                    elif word.endswith('I/Me '):\n",
    "                        word = word.replace('I/Me ', 'me ')\n",
    "                    count_same_frame = 0\n",
    "\n",
    "            elif cv2.contourArea(contour) < 1000:\n",
    "                if word != '':\n",
    "                    #print('yolo')\n",
    "                    #say_text(text)\n",
    "                    Thread(target=say_text, args=(word, )).start()\n",
    "                text = \"\"\n",
    "                word = \"\"\n",
    "        else:\n",
    "            if word != '':\n",
    "                #print('yolo1')\n",
    "                #say_text(text)\n",
    "                Thread(target=say_text, args=(word, )).start()\n",
    "            text = \"\"\n",
    "            word = \"\"\n",
    "        blackboard = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "        cv2.putText(blackboard, \" \", (180, 50), cv2.FONT_HERSHEY_TRIPLEX, 1.5, (255, 0,0))\n",
    "        cv2.putText(blackboard, \"Predicted text- \" + text, (30, 100), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 255, 0))\n",
    "        cv2.putText(blackboard, word, (30, 240), cv2.FONT_HERSHEY_TRIPLEX, 2, (255, 255, 255))\n",
    "        if is_voice_on:\n",
    "            cv2.putText(blackboard, \"Voice ON\", (450, 440), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 127, 0))\n",
    "        else:\n",
    "            cv2.putText(blackboard, \"Voice OFF\", (450, 440), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 127, 0))\n",
    "        cv2.rectangle(img, (x,y), (x+w, y+h), (0,255,0), 2)\n",
    "        res = np.hstack((img, blackboard))\n",
    "        cv2.imshow(\"Recognizing gesture\", res)\n",
    "        cv2.imshow(\"thresh\", thresh)\n",
    "        keypress = cv2.waitKey(1)\n",
    "        if keypress == ord('q') or keypress == ord('c'):\n",
    "            break\n",
    "        if keypress == ord('v') and is_voice_on:\n",
    "            is_voice_on = False\n",
    "        elif keypress == ord('v') and not is_voice_on:\n",
    "            is_voice_on = True\n",
    "\n",
    "    if keypress == ord('c'):\n",
    "        return 2\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def recognize():\n",
    "    cam = cv2.VideoCapture(1)\n",
    "    if cam.read()[0]==False:\n",
    "        cam = cv2.VideoCapture(0)\n",
    "    text = \"\"\n",
    "    word = \"\"\n",
    "    count_same_frame = 0\n",
    "    keypress = 1\n",
    "    while True:\n",
    "        if keypress == 1:\n",
    "            keypress = text_mode(cam)\n",
    "        elif keypress == 2:\n",
    "            keypress = calculator_mode(cam)\n",
    "        else:\n",
    "            cam.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "\n",
    "keras_predict(model, np.zeros((50, 50), dtype = np.uint8))\n",
    "recognize()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
